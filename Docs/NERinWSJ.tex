\documentclass[draft]{article}

\usepackage[]{url}
\usepackage[margin=0.6in]{geometry}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{multicol}

\setlength{\columnsep}{0.5in}

\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\setcitestyle{authoryear}


\title{Named Entity Recognition in the WSJ Corpus}
\author{George Taylor}
\date{Semester 1 - 2016/17}


\begin{document}
\nocite{*}


\maketitle
\begin{multicols*}{2}

\section*{Abstract}
The aim of Named Entity Recognition (NER) in this case is to recognise entities that have been referenced in the text and determine the type of the entity from (Person, Location or Organization). The program will be provided with a set of tagged and untagged articles from the WSJ corpus as well as a set of test articles to analyse the preformance of the system.

\section*{Extracting Training Data}
The first task is to extract the already tagged entities from the set of tagged documents which is done using the reqular expression: \begin{verbatim}<ENAMEX TYPE=".*?">.*?</ENAMEX>\end{verbatim} which returns a list of all tags in the documents from which it is trivial to extract the entities and their types.

Once we have the entity within the tags we can part of speech (POS) tag it using \texttt{nltk.tokenize()}, giving us the POS tags for every entity \citep{NLPChunker}.

\section*{Grammar Creation}
The next step of the process is to take these entities and create a grammar that can be used in a chunker \citep{NLPChunker} to extract unseen entities. Firstly, any occurences of ``)'', ``('' or ``:'' are removed as these are reserved characters for the grammar and would cause issues while keeping them would have no forseeable benefit. A list of tuples is then created that gives the POS tags and entity type, e.g. \begin{verbatim}[(["NNP", "NNP"], "PERSON")]\end{verbatim} which are then compiled into a dictionary that gives the frequecy of each tag sequence for each type. From this, the program will immediately remove any tuples that occur below a threshold frequency (currently set to the average frequency multiplied by 2.6). The tags are then sorted using a heuristic that favours their length and then their frequency, thus promoting greedy evaluation. The resulting grammar will be in this format:
\begin{verbatim}
ORGANIZATION: {<NNP><NNP><CC><NNP><NNP>}
ORGANIZATION: {<NNP><NNP><IN><NNP>}
PERSON: {<NNP><NNP><NNP><NNP>}
ORGANIZATION: {<NNP><NNP><NNP><NNP>}
PERSON: {<NNP><NN><NNP>}
ORGANIZATION: {<NNP><NNP><NNPS>}
ORGANIZATION: {<JJ><NNP><NNP>}
LOCATION: {<NNP><NNP><NNP>}
ORGANIZATION: {<NNP><IN><NNP>}
PERSON: {<NNP><NNP>}
ORGANIZATION: {<NNP><NNP>}
PERSON: {<NNP>}
LOCATION: {<NN>}
PERSON: {<NN>}
ORGANIZATION: {<NN>}
LOCATION: {<NNP>}
ORGANIZATION: {<NNP>}
\end{verbatim}

While this grammar will distinguish between different types of entity, it is not accurate enough to be used in reality. For example, because of the presence of rules such as \texttt{PERSON: {<NNP>}} many unwanted nouns will be matched and we can not simply assume that these are all names. For this reason, the grammar is only used to extract entities and the types of these entities are ignored.

\section*{Extra Datasets}
Throughout the program a couple of extra datasets have been used:
\begin{itemize}
\item DBpedia: This was orgininally a downloaded dataset that had been reformatted giving a list of entities and their types. However this has been modified to now use the SPARQL query language to access the entities and their types directly from the online database. Because of the time it takes to ocnduct these online queries this is only done as a last resort if the entity could not be related any other way. The advantage of this method is that the database will be continuously updated so new entities can be identified without having to download any extra data.
\item Names corpus: The nltk package provides a corpus of names, this makes the task of matching names significantly easier as using the corpus alongside some other rules can lead to extremely accurate name matching.
\item IE-ER corpus: The ``Information Extraction and Entity Recognition'' corpus provided by the Information Technology Laboratory division of the NIST provides a corpus of 1506 entities with types Person, Location or Organization, so while small it means that up to 1506 entities can be related almost instantly.
\end{itemize}

\section*{Chunking}
Using the grammar that has now been created, the program can extract entities from the untagged data. By iterating over the files we can get the contents for each file one at a time. The program then uses \texttt{nltk.sent\_tokenize()} then \texttt{nltk.word\_tokenize()} to split the text into words. Using \texttt{nltk.pos\_tag()} and \texttt{nltk.RegexpParser(grammar)} we can get a parse tree of the text that can be iterated over in order to extract the entities. These entities can now be processed individually in order to determine their type. In addition to this, the \texttt{get\_relation()} function takes a list of the ten previously identified entities which are then used to help identify organisations (e.g. if \textit{`Hercules Inc.'} has been identified as an organisation then the next few referrences to \textit{`Hercules'} are likely referring to the same entity). The function also takes the previous word in the sentence, this is used to quickly identify relations as if the entity is preceeded by \textit{`in'} then it is most likely a Location.

\section*{Entity Relations}
Because relating entities has the potential to be extremely time consuming the first step is to apply some of the more obvious rules to identify the type of the entity. The program will first check the corpora to check if it can immediately identify the relation. It will then check the previous word to see if the entity can be related from this. Following this there are three functions that attempt to identify the entity as a specific type (executed in this order):
\begin{itemize}
\item \texttt{is\_organization()}: This will first check if the entity contains any of the words used solely for businesses (\textit{`Inc.', `Corp.', `PLC', etc.}). Failing this, if the entity is in full capitals, has a length less than 7 and is only one word then it is likely to be an abbreviation for a company (this can cause issues with over-generation as it will match entitys like \textit{`GDP'} that are obviously not companies). If this fails then it will go through the previous ten entities checking if the current entity can be found within any of them.
\item \texttt{is\_name()}: This will first check if the entity contains any of the words commonly associated with names, e.g.\textit{`Mr.'}, \textit{`Mrs.'}, \textit{`Jr'}, \textit{`etc'}. If none of these can be found then it will check if the entity contains at least one word that can be found in the names corpus, and the rest of the words are either initials (a single capital letter followed by a period) or start with an uppercase letter.
\item \texttt{is\_location()}: Because most locations will have been matched in the initial stages of getting the relation, most locations that are left over are abbreviations of a location such as \textit{`Del.'} or \textit{`Tex.'}. For this reason the program will return a location if the entity is a single word with at most 7 letters, the first being a capital and the last being a period. 
\end{itemize}

If all of these options are exhausted then the program can attempt to access DBpedia in order to determine the entity. The program will query DBpedia using the SPARQL \citep{SPARQL} query:
\begin{verbatim}
SELECT ?t
WHERE {
	OPTIONAL { 
	<http://DBpedia.org/resource/%s> a ?t 
	} .
}
\end{verbatim}
where \texttt{\%s} is the name of the identifier. This query will return the DBpedia listing for the type of the entity if it can be found. It is then trivial to check if the listing contains any of the required relations. 

\section*{Observations}
As previously mentioned, the grammar is created by taking only those rules that have frequency greater than the average multiplied by 2.6. Testing the program on the test data with different values for the multiplier yields the following results (these results were calculated before extensions to the datasets were made):

\begin{center}
\begin{tabular}{| c || c | c | c |}
\hline
Multiplier & Precision (\%) & Recall (\%) & F\textsubscript{1} \\
\hline
0          & 33.63          & 41.4        & 37.11 \\
%0.1        & 59.24          & 53.51 \\
%0.2        & 63.76          & 59.55 \\
0.4        & 63.56          & 59.71       & 61.57 \\
%0.6        & 63.65          & 61.72 \\
%0.7        & 63.7           & 61.88 \\
0.8        & 63.69          & 61.88       & 62.77 \\
%1.0        & 64.04          & 63.04 \\
1.2        & 63.77          & 63.16       & 63.46 \\
%1.4        & 66.31          & 68.48 \\
1.6        & 67.32          & 70.68       & 68.96 \\
%1.8        & 67.71          & 72.56 \\
2.6        & 69.3           & 75.83       & 72.42 \\
4.0        & 68.99          & 76.66       & 72.62 \\

\hline
\end{tabular}
\end{center}

It is also worth noting that for smaller values of the multiplier, the larger resulting grammar causes the NER to run significantly slower. From this table it is evident that larger values for the multiplier produce better results, however larger values also lead to a considerably longer runtime as a result of a greater dependence on online resources.

\section*{Results}
The process for completing the named entity recognition is as follows:
\begin{enumerate}
\item Load the entities from the tagged training data.
\item Create a grammar from these entities.
\item Optionally test the grammar on the untagged training data and gather statistics.
\item Test the grammar on the untagged test data.
\item Load the entities from the tagged test data and calculate statistics for the NER performance.
\end{enumerate}
When testing on the untagged training data, the program can produce a precision of 73.02\% and recall of 83.16\%. However because the program stores the entities and relations loaded from the tagged training data, this is a somewhat inflated result. The entities in the untagged data are common to those loaded from the tagged data giving it an inherant advantage. When completing NER on the test data the program can still achieve a precision of 69.3\% and recall of 75.83\%. While this is noticably lower it is still sufficient to conclude that the program is successful when being run on unseen data.

\bibliography{NERinWSJ_bib}

\end{multicols*}


\end{document}